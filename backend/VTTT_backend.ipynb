{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOW50tZ6G16Y"
      },
      "source": [
        "## **Voice-to-text Translator Backend**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Ck8mcrJI-X"
      },
      "source": [
        "**Loading DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L25-2JFgG0_X",
        "outputId": "5274c0a0-e4c9-4d10-cdb1-b074456def59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-04 08:44:00--  https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-ur.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1218189 (1.2M) [application/zip]\n",
            "Saving to: ‘TED2020.en-ur.zip’\n",
            "\n",
            "TED2020.en-ur.zip   100%[===================>]   1.16M  1.13MB/s    in 1.0s    \n",
            "\n",
            "2024-12-04 08:44:03 (1.13 MB/s) - ‘TED2020.en-ur.zip’ saved [1218189/1218189]\n",
            "\n",
            "Archive:  TED2020.en-ur.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: TED2020.en-ur.en        \n",
            "  inflating: TED2020.en-ur.ur        \n",
            "  inflating: TED2020.en-ur.xml       \n"
          ]
        }
      ],
      "source": [
        "!wget -O TED2020.en-ur.zip https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-ur.txt.zip\n",
        "!unzip TED2020.en-ur.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HOBcXF_G0oH",
        "outputId": "7cc1d43b-3fc2-458d-8378-e7056cc0ee90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preprocessing complete. Saved as 'ted_talks_english_urdu.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_ted_dataset(en_file, ur_file):\n",
        "    with open(en_file, 'r', encoding='utf-8') as en, open(ur_file, 'r', encoding='utf-8') as ur:\n",
        "        english_sentences = en.readlines()\n",
        "        urdu_sentences = ur.readlines()\n",
        "\n",
        "    assert len(english_sentences) == len(urdu_sentences), \"Files line counts don't match.\"\n",
        "    data = {\"english\": english_sentences, \"urdu\": urdu_sentences}\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Updated file paths based on your screenshot\n",
        "en_file_path = \"TED2020.en-ur.en\"\n",
        "ur_file_path = \"TED2020.en-ur.ur\"\n",
        "\n",
        "# Load the dataset\n",
        "df = load_ted_dataset(en_file_path, ur_file_path)\n",
        "\n",
        "# Clean and save\n",
        "df['english'] = df['english'].str.strip()\n",
        "df['urdu'] = df['urdu'].str.strip()\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Save to a CSV file\n",
        "df.to_csv(\"ted_talks_english_urdu.csv\", index=False)\n",
        "\n",
        "print(\"Dataset preprocessing complete. Saved as 'ted_talks_english_urdu.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKp2CODmI2-U"
      },
      "source": [
        "**Installing relevant dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x1QyYdgLqDG",
        "outputId": "9f8b7a6f-3aae-4d5f-f972-5c5850411eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=6bdad138f4b83362c55e9a301f425a32017119e34088ef7a78d09dd7d07e41fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/7c/1d/015619716e2facae6631312503baf3c3220e6a9a3508cb14b6\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper, uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.6 pyngrok-7.2.1 starlette-0.41.3 uvicorn-0.32.1 whisper-1.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hgCq_SHMM7B",
        "outputId": "f1ea88e7-85ba-4558-b862-47b394d3ddfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeakrV7_Mi3y",
        "outputId": "e481af80-96bc-47c7-e06a-57208866d7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "P3i68NUXM5ii",
        "outputId": "96b16e85-c89a-4c62-ed22-a8a6cc44ae03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=ed37e24cde629dae830eb29916777bf63905052ef7bedc971e589d8e61dc3592\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "whisper"
                ]
              },
              "id": "55f5fa59cf37477b9700cfccdca09f63"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEzMP0vUNiVC",
        "outputId": "8c2bdc98-9528-482f-d17a-7b252cc64bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.19\n"
          ]
        }
      ],
      "source": [
        "!pip install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBAe8aYVO0Ls",
        "outputId": "04381af0-283c-48ad-cbd3-369053d3389b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2opx3iwAs2jYTIWTw7r0pEyXevX_2YtqaJv3nMDqLk7Nq2i3K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qae8YJGAz6jw"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y socat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALl8hbNLJ39i",
        "outputId": "26a70916-6656-4323-fca8-2b8d96e05b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.6)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.41.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsIuJQxpQfNP",
        "outputId": "f1a98372-0557-4055-e7af-09013f8c8185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28.0) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.4\n",
            "    Uninstalling openai-1.54.4:\n",
            "      Successfully uninstalled openai-1.54.4\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wW6FKPRQomX"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yl4vzIRpG5k"
      },
      "source": [
        "**Model training and data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCLxWzIIpB9z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import os\n",
        "\n",
        "# Disable WANDB for now\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check if CUDA (GPU) is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files={\"train\": \"ted_talks_english_urdu.csv\"})\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "# Ensure no None or empty values\n",
        "dataset = dataset.filter(lambda example: example['english'] and example['urdu'])\n",
        "\n",
        "# Load Pretrained MBart50 model and tokenizer\n",
        "model_name = \"abdulwaheed1/english-to-urdu-translation-mbart\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"ur_PK\", tgt_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)  # Move model to GPU if available\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [text for text in examples['urdu']]  # Urdu as source\n",
        "    targets = [text for text in examples['english']]  # English as target\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=150, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=150, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    labels = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_total_limit=2,\n",
        "    generation_max_length=150,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    # Use mixed precision for faster training\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./urdu_to_english_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./urdu_to_english_finetuned_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS0Un5keo-uc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "import os\n",
        "\n",
        "# Disable WANDB for now\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check if CUDA (GPU) is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files={\"train\": \"ted_talks_english_urdu.csv\"})\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "# Ensure no None or empty values\n",
        "dataset = dataset.filter(lambda example: example['english'] and example['urdu'])\n",
        "\n",
        "# Load Pretrained MBart50 model and tokenizer\n",
        "model_name = \"abdulwaheed1/english-to-urdu-translation-mbart\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"ur_PK\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)  # Move model to GPU if available\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Prepare the translation inputs for the MarianMT model\n",
        "    inputs = [text for text in examples['english']]\n",
        "    targets = [text for text in examples['urdu']]\n",
        "\n",
        "    # Tokenize inputs and labels with padding\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    # Replace padding token ID for labels with -100 (ignored in loss calculation)\n",
        "    labels = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    save_total_limit=2,\n",
        "    generation_max_length=128,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    # Use mixed precision for faster training\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./ted_talks_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./ted_talks_finetuned_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn07OTGYJf97"
      },
      "source": [
        "**Loading model from google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3mMYf5HNeSs",
        "outputId": "36f9250f-7173-4b9b-a1dd-b3ab13e5474d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UOvDMvUNldk"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvTh_DbcNuFO"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/NLP_Translator_modle/ted_talks_finetuned_model.zip /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCwn03X_Nyfc",
        "outputId": "8ca3f914-410f-4f03-faf2-f608d65361ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ted_talks_finetuned_model.zip\n",
            "   creating: ted_talks_finetuned_model/\n",
            "  inflating: ted_talks_finetuned_model/config.json  \n",
            "  inflating: ted_talks_finetuned_model/generation_config.json  \n",
            "  inflating: ted_talks_finetuned_model/model.safetensors  \n",
            "  inflating: ted_talks_finetuned_model/sentencepiece.bpe.model  \n",
            "  inflating: ted_talks_finetuned_model/special_tokens_map.json  \n",
            "  inflating: ted_talks_finetuned_model/tokenizer.json  \n",
            "  inflating: ted_talks_finetuned_model/tokenizer_config.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip ted_talks_finetuned_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi5t6baakbt4"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/NLP_Translator_modle/urdu_to_english_finetuned_model.zip /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uss5VfdhlBgz",
        "outputId": "3dce6db2-2f70-4b1c-9427-a6539704e833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  urdu_to_english_finetuned_model.zip\n",
            "   creating: urdu_to_english_finetuned_model/\n",
            "  inflating: urdu_to_english_finetuned_model/config.json  \n",
            "  inflating: urdu_to_english_finetuned_model/generation_config.json  \n",
            "  inflating: urdu_to_english_finetuned_model/model.safetensors  \n",
            "  inflating: urdu_to_english_finetuned_model/sentencepiece.bpe.model  \n",
            "  inflating: urdu_to_english_finetuned_model/special_tokens_map.json  \n",
            "  inflating: urdu_to_english_finetuned_model/tokenizer.json  \n",
            "  inflating: urdu_to_english_finetuned_model/tokenizer_config.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip urdu_to_english_finetuned_model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TQdhcaSzBx"
      },
      "source": [
        "**Model Evalution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ4DS3ZBf4rz"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F91EKTXgUp-"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvRo9noCSfF-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from evaluate import load\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load TED Talks dataset\n",
        "dataset = load_dataset('csv', data_files={\"train\": \"ted_talks_english_urdu.csv\"})\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "test_dataset = dataset['test']  # Use the test split for evaluation\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_name = \"./urdu_to_english_finetuned_model\"  # Path to your model\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"ur_PK\", tgt_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize BLEU score and F1 score metrics\n",
        "bleu_metric = load(\"bleu\")\n",
        "\n",
        "# Function to translate Urdu to English\n",
        "def translate_urdu_to_english(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=150, truncation=True, padding=\"max_length\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move tensors to the correct device\n",
        "    outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Limit the number of samples to evaluate\n",
        "sample_size = 1\n",
        "df_sampled = test_dataset.select(range(sample_size))  # Select first `sample_size` samples\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Iterate over the sampled test data\n",
        "for row in df_sampled:\n",
        "    urdu_text = row['urdu']\n",
        "    reference_translation = row['english']\n",
        "\n",
        "    # Skip rows with missing or empty Urdu text\n",
        "    if not isinstance(urdu_text, str) or not urdu_text.strip():\n",
        "        continue\n",
        "\n",
        "    # Translate and store predictions and references\n",
        "    prediction = translate_urdu_to_english(urdu_text)\n",
        "    predictions.append(prediction)\n",
        "    references.append([reference_translation])  # BLEU metric expects a list of references\n",
        "\n",
        "\n",
        "# Ensure predictions and references are of the same length\n",
        "assert len(predictions) == len(references), f\"Length mismatch: {len(predictions)} != {len(references)}\"\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n",
        "print(f\"BLEU Score: {bleu_score['bleu']}\")\n",
        "\n",
        "# Convert predictions and references to tokens for F1 score calculation\n",
        "def tokenize_sentences(sentences):\n",
        "    return [sentence.split() for sentence in sentences]\n",
        "\n",
        "tokenized_predictions = tokenize_sentences(predictions)\n",
        "tokenized_references = tokenize_sentences([ref[0] for ref in references])\n",
        "\n",
        "# Flatten the tokenized lists\n",
        "flat_predictions = [item for sublist in tokenized_predictions for item in sublist]\n",
        "flat_references = [item for sublist in tokenized_references for item in sublist]\n",
        "\n",
        "# Calculate F1 score (micro average)\n",
        "f1 = f1_score(flat_references, flat_predictions, average='micro')\n",
        "print(f\"F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XDKeLTbAjLK"
      },
      "source": [
        "**API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zr7RpS-CqIN",
        "outputId": "bbe1189a-b631-41d7-9960-460cb43c1f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI is accessible at: NgrokTunnel: \"https://8607-34-83-86-66.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [4159]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:ba80:ac93:358:bc20:6020:7497:76a6:0 - \"OPTIONS /upload-audio HTTP/1.1\" 200 OK\n",
            "INFO:     2401:ba80:ac93:358:bc20:6020:7497:76a6:0 - \"POST /upload-audio HTTP/1.1\" 200 OK\n",
            "INFO:     2401:ba80:ac93:358:bc20:6020:7497:76a6:0 - \"POST /upload-audio HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from fastapi.responses import JSONResponse\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "\n",
        "# Apply the nest_asyncio patch\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "whisper_model = whisper.load_model(\"medium\")\n",
        "\n",
        "# Load the fine-tuned models and tokenizers\n",
        "# English to Urdu\n",
        "model_path_eng_to_urd = \"./ted_talks_finetuned_model\"\n",
        "tokenizer_eng_to_urd = MBart50TokenizerFast.from_pretrained(model_path_eng_to_urd, src_lang=\"en_XX\", tgt_lang=\"ur_PK\")\n",
        "model_eng_to_urd = MBartForConditionalGeneration.from_pretrained(model_path_eng_to_urd)\n",
        "\n",
        "# Urdu to English\n",
        "model_path_urd_to_eng = \"./urdu_to_english_finetuned_model\"\n",
        "tokenizer_urd_to_eng = MBart50TokenizerFast.from_pretrained(model_path_urd_to_eng, src_lang=\"ur_PK\", tgt_lang=\"en_XX\")\n",
        "model_urd_to_eng = MBartForConditionalGeneration.from_pretrained(model_path_urd_to_eng)\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_eng_to_urd.to(device)\n",
        "model_urd_to_eng.to(device)\n",
        "\n",
        "def translate_text(text, direction):\n",
        "    \"\"\"Handles translation based on the specified direction.\"\"\"\n",
        "    if direction == \"eng-to-urd\":\n",
        "        tokenizer, model = tokenizer_eng_to_urd, model_eng_to_urd\n",
        "    elif direction == \"urd-to-eng\":\n",
        "        tokenizer, model = tokenizer_urd_to_eng, model_urd_to_eng\n",
        "    else:\n",
        "        raise ValueError(\"Invalid translation direction!\")\n",
        "\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=150, truncation=True, padding=\"max_length\")\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}  # Move tensors to the correct device\n",
        "\n",
        "    # Generate translation\n",
        "    outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Configure CORS middleware to accept requests from any origin\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allows all origins; specify frontend URL if needed\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allows all methods (GET, POST, etc.)\n",
        "    allow_headers=[\"*\"],  # Allows all headers\n",
        ")\n",
        "\n",
        "# Create a tunnel to the FastAPI app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"FastAPI is accessible at:\", public_url)\n",
        "\n",
        "# Define a Pydantic model to handle the message data\n",
        "class Message(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# Root endpoint (for testing)\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the FastAPI app!\"}\n",
        "\n",
        "# Hello World endpoint (for testing)\n",
        "@app.get(\"/hello\")\n",
        "async def hello_world():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "# Favicon endpoint (to avoid 404 error for GET /favicon.ico)\n",
        "@app.get(\"/favicon.ico\")\n",
        "async def favicon():\n",
        "    return JSONResponse(content={})\n",
        "\n",
        "# Endpoint to accept the message from the frontend via POST\n",
        "@app.post(\"/send-message\")\n",
        "async def send_message(data: Message):\n",
        "    return {\"received_message\": data.message}\n",
        "\n",
        "# Endpoint to upload audio data and transcribe with Whisper\n",
        "@app.post(\"/upload-audio\")\n",
        "async def upload_audio(request: Request):\n",
        "    try:\n",
        "        audio_data = await request.body()  # Read binary data\n",
        "        direction = request.headers.get(\"Translation-Direction\", None)\n",
        "\n",
        "        if not direction or direction not in [\"eng-to-urd\", \"urd-to-eng\"]:\n",
        "            return JSONResponse(\n",
        "                content={\"error\": \"Invalid or missing Translation-Direction header\"},\n",
        "                status_code=400\n",
        "            )\n",
        "\n",
        "        # Save the audio data temporarily as an Opus file\n",
        "        opus_path = \"temp_audio.opus\"\n",
        "        with open(opus_path, \"wb\") as f:\n",
        "            f.write(audio_data)\n",
        "\n",
        "        # Define the WAV file path\n",
        "        wav_path = \"temp_audio.wav\"\n",
        "\n",
        "        # Convert Opus to WAV using ffmpeg command line\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"ffmpeg\", \"-y\", \"-i\", opus_path, wav_path],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE\n",
        "            )\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            error_message = e.stderr.decode()\n",
        "            print(f\"FFmpeg error: {error_message}\")\n",
        "            return JSONResponse(\n",
        "                content={\"error\": \"Audio conversion failed\", \"details\": error_message},\n",
        "                status_code=500\n",
        "            )\n",
        "\n",
        "        # Transcribe the audio using the local Whisper model\n",
        "        try:\n",
        "          if direction == \"eng-to-urd\":\n",
        "            result = whisper_model.transcribe(wav_path)\n",
        "            transcription = result[\"text\"]\n",
        "          else:\n",
        "            result = whisper_model.transcribe(wav_path, language=\"Urdu\")\n",
        "            transcription = result[\"text\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Whisper transcription error: {str(e)}\")\n",
        "            return JSONResponse(\n",
        "                content={\"error\": \"Transcription failed\", \"details\": str(e)},\n",
        "                status_code=500\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            translation = translate_text(transcription, direction)\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {str(e)}\")\n",
        "            translation = \"Translation unavailable\"\n",
        "\n",
        "\n",
        "        # Cleanup temporary files\n",
        "        os.remove(opus_path)\n",
        "        os.remove(wav_path)\n",
        "\n",
        "        # Return the transcription result\n",
        "        return {\"message\": \"Audio received\", \"transcription\": transcription, \"translation\" : translation}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"General error: {str(e)}\")\n",
        "        return JSONResponse(\n",
        "            content={\"error\": \"Unexpected error occurred\", \"details\": str(e)},\n",
        "            status_code=500\n",
        "        )\n",
        "\n",
        "# To run the server with FastAPI in Jupyter/Colab\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "cLCXNIUdKINk"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}